{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PasinduWaidyarathna/Deep-Learning-Mini-Project-03/blob/main/test03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary Library Imports"
      ],
      "metadata": {
        "id": "QT629icZcaSl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Fc2KeNtcFLn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Google Drive"
      ],
      "metadata": {
        "id": "UZX0slNVcbgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P7AydiWcbxY",
        "outputId": "8b112698-a146-4993-989b-886cba2a426a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data file"
      ],
      "metadata": {
        "id": "_ik52dtbcb7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"/content/drive/MyDrive/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "   lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "i = 0\n",
        "for line in lines:\n",
        "   print(line)\n",
        "   i = i + 1\n",
        "   if(i==20):\n",
        "     break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qWYVkWpccEo",
        "outputId": "3b1e2027-5776-4ba6-b52b-e8f140baca60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVe.\n",
            "Go.\tVete.\n",
            "Go.\tVaya.\n",
            "Go.\tVáyase.\n",
            "Hi.\tHola.\n",
            "Run!\t¡Corre!\n",
            "Run.\tCorred.\n",
            "Who?\t¿Quién?\n",
            "Fire!\t¡Fuego!\n",
            "Fire!\t¡Incendio!\n",
            "Fire!\t¡Disparad!\n",
            "Help!\t¡Ayuda!\n",
            "Help!\t¡Socorro! ¡Auxilio!\n",
            "Help!\t¡Auxilio!\n",
            "Jump!\t¡Salta!\n",
            "Jump.\tSalte.\n",
            "Stop!\t¡Parad!\n",
            "Stop!\t¡Para!\n",
            "Stop!\t¡Pare!\n",
            "Wait!\t¡Espera!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARq2YoywccVN",
        "outputId": "03020650-ebbd-4614-b598-76a6fa1cc053"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can't view Flash content on an iPad. However, you can easily email yourself the URLs of these web pages and view that content on your regular computer when you get home.\tNo puedes ver contenido en Flash en un iPad. Sin embargo, puedes fácilmente enviarte por correo electrónico las URL's de esas páginas web y ver el contenido en tu computadora cuando llegas a casa.\n",
            "A mistake young people often make is to start learning too many languages at the same time, as they underestimate the difficulties and overestimate their own ability to learn them.\tUn error que cometen a menudo los jóvenes es el de comenzar a aprender demasiadas lenguas al mismo tiempo, porque subestiman sus dificultades y sobrestiman sus propias capacidades para aprenderlas.\n",
            "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\tNo importa cuánto insistas en convencer a la gente de que el chocolate es vainilla, seguirá siendo chocolate, aunque puede que te convenzas a ti mismo y a algunos otros de que es vainilla.\n",
            "In 1969, Roger Miller recorded a song called \"You Don't Want My Love.\" Today, this song is better known as \"In the Summer Time.\" It's the first song he wrote and sang that became popular.\tEn 1969, Roger Miller grabó una canción llamada \"Tú no quieres mi amor\". Hoy, esta canción es más conocida como \"En el verano\". Es la primera canción que escribió y cantó que se convirtió popular.\n",
            "A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.\tUn niño que es hablante nativo normalmente sabe muchas cosas acerca de su lengua que un hablante no nativo que lo haya estado estudiando durante muchos años no sabe todavía y que quizá no sabrá nunca.\n",
            "There are four main causes of alcohol-related death. Injury from car accidents or violence is one. Diseases like cirrhosis of the liver, cancer, heart and blood system diseases are the others.\tHay cuatro causas principales de muertes relacionadas con el alcohol. Lesión por un accidente automovilístico o violencia es una. Enfermedades como cirrosis del hígado, cáncer, enfermedades del corazón y del sistema circulatorio son las otras.\n",
            "There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college education.\tHay madres y padres que se quedan despiertos después de que sus hijos se hayan dormido y se preguntan cómo conseguir pagar la hipoteca o las facturas del médico, o cómo ahorrar el suficiente dinero para la educación universitaria de sus hijos.\n",
            "A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climate change.\tUna huella de carbono es la cantidad de contaminación de dióxido de carbono que producimos como producto de nuestras actividades. Algunas personas intentan reducir su huella de carbono porque están preocupados acerca del cambio climático.\n",
            "Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\tComo suele haber varias páginas web sobre cualquier tema, normalmente sólo le doy al botón de retroceso cuando entro en una página web que tiene anuncios en ventanas emergentes. Simplemente voy a la siguiente página encontrada por Google y espero encontrar algo menos irritante.\n",
            "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\tSi quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the English and Spanish translation pairs"
      ],
      "metadata": {
        "id": "uUtteFotccdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "   english, spanish = line.split(\"\\t\")\n",
        "   spanish = \"[start] \" + spanish + \" [end]\"\n",
        "   text_pairs.append((english, spanish))\n",
        "\n",
        "\n",
        "for i in range(3):\n",
        "  print(random.choice(text_pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3PhpQHtccmD",
        "outputId": "3df822a4-ae02-48e0-c6b6-1d4a3c348c19"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('He has more money than is needed.', '[start] Él tiene más dinero de lo que se necesita. [end]')\n",
            "('When does your trip begin?', '[start] ¿Cuándo empieza tu viaje? [end]')\n",
            "('Married people are happier than unmarried people.', '[start] La gente casada es más feliz que la gente soltera. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize the data"
      ],
      "metadata": {
        "id": "LXZhHwAvccuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "ntlhKik_cc26"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting the data into training, validation and Testing"
      ],
      "metadata": {
        "id": "LTCO23TLcc93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-n5GsD7cdFn",
        "outputId": "9477f94e-0e5a-4d28-9bfd-ea5c092fd871"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 118964\n",
            "Training set size: 83276\n",
            "Validation set size: 17844\n",
            "Testing set size: 17844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_pairs)+len(val_pairs)+len(test_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlz7_hQBcdTu",
        "outputId": "28756d77-b4c5-43ff-ee1a-dc315a3566b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118964"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Punctuations"
      ],
      "metadata": {
        "id": "mfKFGFWTkifQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "f\"[{re.escape(strip_chars)}]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IZrUBdcDcdiB",
        "outputId": "dbc6ea82-d0b9-4695-eba8-90cf58acaafc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{3+5}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G58-tsGTcd0-",
        "outputId": "cf2e59d9-55a6-4b6e-8985-27af81d90ef8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing the English and Spanish text pairs"
      ],
      "metadata": {
        "id": "u9N4er4ccd9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(\n",
        "      lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)\n"
      ],
      "metadata": {
        "id": "m2VJVtguceFs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing datasets for the translation task"
      ],
      "metadata": {
        "id": "xu8bHjxmceNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "   eng = source_vectorization(eng)\n",
        "   spa = target_vectorization(spa)\n",
        "   return ({\n",
        "         \"english\": eng,\n",
        "         \"spanish\": spa[:, :-1],\n",
        "   }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "   print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "   print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "   print(f\"targets.shape: {targets.shape}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsStSjhAceVI",
        "outputId": "80ffc4a5-161f-4551-90db-12bb4977e8bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(train_ds.as_numpy_iterator())[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-M6gEmFcekl",
        "outputId": "6073dd45-4ea2-4430-a34a-2dede554bd90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'english': array([[  82,  211,    6, ...,    0,    0,    0],\n",
            "       [   2, 8526, 4518, ...,    0,    0,    0],\n",
            "       [ 692,   41,   15, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   3,   16,    4, ...,    0,    0,    0],\n",
            "       [  15,    5,  115, ...,    0,    0,    0],\n",
            "       [  45, 2170,   20, ...,    0,    0,    0]]), 'spanish': array([[   2,   81, 1977, ...,    0,    0,    0],\n",
            "       [   2,   32, 6300, ...,    0,    0,    0],\n",
            "       [   2,   42,   17, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,   46,    5, ...,    0,    0,    0],\n",
            "       [   2,    4,  108, ...,    0,    0,    0],\n",
            "       [   2,   75, 8259, ...,    0,    0,    0]])}, array([[   81,  1977,     6, ...,     0,     0,     0],\n",
            "       [   32,  6300, 13872, ...,     0,     0,     0],\n",
            "       [   42,    17,    71, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [   46,     5,    53, ...,     0,     0,     0],\n",
            "       [    4,   108,   342, ...,     0,     0,     0],\n",
            "       [   75,  8259,  2315, ...,     0,     0,     0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer encoder implemented as a subclassed Layer"
      ],
      "metadata": {
        "id": "TPb7zyyocesc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "      self.embed_dim = embed_dim\n",
        "      self.dense_dim = dense_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.attention = layers.MultiHeadAttention(\n",
        "           num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.dense_proj = keras.Sequential(\n",
        "           [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),]\n",
        "      )\n",
        "      self.layernorm_1 = layers.LayerNormalization()\n",
        "      self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "      if mask is not None:\n",
        "         mask = mask[:, tf.newaxis, :]\n",
        "      attention_output = self.attention(\n",
        "         inputs, inputs, attention_mask=mask)\n",
        "      proj_input = self.layernorm_1(inputs + attention_output)\n",
        "      proj_output = self.dense_proj(proj_input)\n",
        "      return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "Xj6kbzgIcez0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer decoder"
      ],
      "metadata": {
        "id": "fub6n634ce7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "      self.embed_dim = embed_dim\n",
        "      self.dense_dim = dense_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.dense_proj = keras.Sequential(\n",
        "          [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "           layers.Dense(embed_dim),]\n",
        "      )\n",
        "      self.layernorm_1 = layers.LayerNormalization()\n",
        "      self.layernorm_2 = layers.LayerNormalization()\n",
        "      self.layernorm_3 = layers.LayerNormalization()\n",
        "      self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "          \"embed_dim\": self.embed_dim,\n",
        "          \"num_heads\": self.num_heads,\n",
        "          \"dense_dim\": self.dense_dim,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "      input_shape = tf.shape(inputs)\n",
        "      batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "      i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "      j = tf.range(sequence_length)\n",
        "      mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "      mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "      mult = tf.concat(\n",
        "              [tf.expand_dims(batch_size, -1),\n",
        "               tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "      return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "      causal_mask = self.get_causal_attention_mask(inputs)\n",
        "      if mask is not None:\n",
        "           padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "           padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "      else:\n",
        "           padding_mask = mask\n",
        "      attention_output_1 = self.attention_1(\n",
        "           query=inputs,\n",
        "           value=inputs,\n",
        "           key=inputs,\n",
        "           attention_mask=causal_mask)\n",
        "      attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "      attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "       )\n",
        "      attention_output_2 = self.layernorm_2(\n",
        "          attention_output_1 + attention_output_2)\n",
        "      proj_output = self.dense_proj(attention_output_2)\n",
        "      return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "rjCAl5PycfDe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding"
      ],
      "metadata": {
        "id": "dEXwSKpfcfKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "     def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "         super().__init__(**kwargs)\n",
        "         self.token_embeddings = layers.Embedding(\n",
        "             input_dim=input_dim, output_dim=output_dim)\n",
        "         self.position_embeddings = layers.Embedding(\n",
        "             input_dim=sequence_length, output_dim=output_dim)\n",
        "         self.sequence_length = sequence_length\n",
        "         self.input_dim = input_dim\n",
        "         self.output_dim = output_dim\n",
        "\n",
        "     def call(self, inputs):\n",
        "         length = tf.shape(inputs)[-1]\n",
        "         positions = tf.range(start=0, limit=length, delta=1)\n",
        "         embedded_tokens = self.token_embeddings(inputs)\n",
        "         embedded_positions = self.position_embeddings(positions)\n",
        "         return embedded_tokens + embedded_positions\n",
        "\n",
        "     def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "     def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "             \"output_dim\": self.output_dim,\n",
        "             \"sequence_length\": self.sequence_length,\n",
        "             \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "XlHKt-ticfTu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End-to-end Transformer"
      ],
      "metadata": {
        "id": "JaRS3BJRcfbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "CBydQxTPcfkC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbvhUH8fcf0E",
        "outputId": "83197df1-d5ce-47b2-ffcf-ea70f638cc3a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " spanish (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['spanish[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the sequence-to-sequence Transformer"
      ],
      "metadata": {
        "id": "izcfAoCCcf7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtD5TtarcgFw",
        "outputId": "24fca460-1a0f-41f2-9feb-8569d65c0b67"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - 109s 76ms/step - loss: 3.7981 - accuracy: 0.4396 - val_loss: 2.8855 - val_accuracy: 0.5337\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 2.8583 - accuracy: 0.5486 - val_loss: 2.5145 - val_accuracy: 0.5891\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 2.5612 - accuracy: 0.5928 - val_loss: 2.3841 - val_accuracy: 0.6131\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 2.3982 - accuracy: 0.6188 - val_loss: 2.3110 - val_accuracy: 0.6266\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 2.2944 - accuracy: 0.6371 - val_loss: 2.2977 - val_accuracy: 0.6310\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 2.2184 - accuracy: 0.6513 - val_loss: 2.2828 - val_accuracy: 0.6397\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 2.1597 - accuracy: 0.6628 - val_loss: 2.2514 - val_accuracy: 0.6474\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 2.0964 - accuracy: 0.6757 - val_loss: 2.2468 - val_accuracy: 0.6502\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 2.0407 - accuracy: 0.6858 - val_loss: 2.2439 - val_accuracy: 0.6552\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.9945 - accuracy: 0.6940 - val_loss: 2.2587 - val_accuracy: 0.6549\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.9543 - accuracy: 0.7018 - val_loss: 2.2242 - val_accuracy: 0.6635\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.9258 - accuracy: 0.7070 - val_loss: 2.2077 - val_accuracy: 0.6668\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.8960 - accuracy: 0.7122 - val_loss: 2.2406 - val_accuracy: 0.6667\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.8735 - accuracy: 0.7167 - val_loss: 2.2750 - val_accuracy: 0.6612\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.8518 - accuracy: 0.7205 - val_loss: 2.2666 - val_accuracy: 0.6669\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - 92s 70ms/step - loss: 1.8317 - accuracy: 0.7245 - val_loss: 2.3091 - val_accuracy: 0.6587\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.8146 - accuracy: 0.7275 - val_loss: 2.3174 - val_accuracy: 0.6602\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.7949 - accuracy: 0.7306 - val_loss: 2.2788 - val_accuracy: 0.6688\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.7776 - accuracy: 0.7340 - val_loss: 2.3129 - val_accuracy: 0.6656\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.7598 - accuracy: 0.7373 - val_loss: 2.3814 - val_accuracy: 0.6632\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.7464 - accuracy: 0.7391 - val_loss: 2.3210 - val_accuracy: 0.6675\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.7321 - accuracy: 0.7421 - val_loss: 2.3262 - val_accuracy: 0.6681\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.7192 - accuracy: 0.7442 - val_loss: 2.3343 - val_accuracy: 0.6701\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.7022 - accuracy: 0.7473 - val_loss: 2.3548 - val_accuracy: 0.6635\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.6887 - accuracy: 0.7497 - val_loss: 2.3409 - val_accuracy: 0.6729\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.6774 - accuracy: 0.7523 - val_loss: 2.3844 - val_accuracy: 0.6687\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.6665 - accuracy: 0.7543 - val_loss: 2.4127 - val_accuracy: 0.6631\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.6541 - accuracy: 0.7563 - val_loss: 2.3774 - val_accuracy: 0.6710\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.6437 - accuracy: 0.7583 - val_loss: 2.4376 - val_accuracy: 0.6636\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - 91s 70ms/step - loss: 1.6322 - accuracy: 0.7599 - val_loss: 2.4022 - val_accuracy: 0.6691\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79bcd4101930>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translating new sentences with our Transformer model"
      ],
      "metadata": {
        "id": "HtV-4Jp7cgNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n"
      ],
      "metadata": {
        "id": "lKCdvEQFcgWn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output Testing and Decoding the output sequence"
      ],
      "metadata": {
        "id": "P124OmIxcgeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "     tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "     decoded_sentence = \"[start]\"\n",
        "     for i in range(max_decoded_sentence_length):\n",
        "       tokenized_target_sentence = target_vectorization(\n",
        "         [decoded_sentence])[:, :-1]\n",
        "       predictions = transformer(\n",
        "         [tokenized_input_sentence, tokenized_target_sentence])\n",
        "       sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "       sampled_token = spa_index_lookup[sampled_token_index]\n",
        "       decoded_sentence += \" \" + sampled_token\n",
        "       if sampled_token == \"[end]\": break\n",
        "     return decoded_sentence"
      ],
      "metadata": {
        "id": "xjO9rIZkcgnq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer translating output"
      ],
      "metadata": {
        "id": "LzPCHf-Acgvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmWCUkuscg4y",
        "outputId": "64daaf37-0e83-4326-dbb0-39e16ae637ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Physics is a branch of science.\n",
            "[start] de física es un arma de la ciencia [end]\n",
            "-\n",
            "I lost my watch.\n",
            "[start] perdí a mi reloj [end]\n",
            "-\n",
            "He often accepted bad advice.\n",
            "[start] Él a menudo aceptó su novia [end]\n",
            "-\n",
            "I don't know how to interpret his words.\n",
            "[start] no sé cómo se [UNK] sus palabras [end]\n",
            "-\n",
            "We should remain quiet.\n",
            "[start] deberíamos seguir hacia silencio [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation using the BLEU score"
      ],
      "metadata": {
        "id": "r660q0N3chCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "test_spa_texts = [pair[1] for pair in test_pairs]\n",
        "score = 0\n",
        "bleu  = 0\n",
        "for i in range(20):\n",
        "    candidate = decode_sequence(test_eng_texts[i])\n",
        "    reference = test_spa_texts[i].lower()\n",
        "    print(candidate,reference)\n",
        "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu+=score\n",
        "    print(f\"Score:{score}\")\n",
        "print(f\"\\nBLEU score : {round(bleu,2)}/20\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs6FBdvQchK-",
        "outputId": "c3acf4a4-dfbf-423d-f46b-7869aa49030e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] qué tiene que hacer conmigo [end] [start] ¿qué tiene que ver eso conmigo? [end]\n",
            "Score:0.43902439024390244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] los lunes no son buenos [end] [start] el lunes no va bien. [end]\n",
            "Score:0.3783783783783784\n",
            "[start] tom es una [UNK] [end] [start] tom es torpe. [end]\n",
            "Score:0.4\n",
            "[start] no puedes [UNK] [end] [start] no puedes perdértelo. [end]\n",
            "Score:0.4482758620689655\n",
            "[start] tiene que estar borracho demasiado anoche [end] [start] él debe haber bebido demasiado anoche. [end]\n",
            "Score:0.2909090909090909\n",
            "[start] tom nunca come [UNK] [end] [start] tom nunca come quiché. [end]\n",
            "Score:0.4117647058823529\n",
            "[start] me [UNK] el sombrero que le [UNK] por el viento [end] [start] el viento se llevó mi sombrero. [end]\n",
            "Score:0.26229508196721313\n",
            "[start] sabes que algo no sé [end] [start] ¿sabes tú algo que yo no sepa? [end]\n",
            "Score:0.47058823529411764\n",
            "[start] crees en los [UNK] [end] [start] ¿crees en los ángeles? [end]\n",
            "Score:0.40625\n",
            "[start] me [UNK] [end] [start] yo cuidaré a los caballos. [end]\n",
            "Score:0.45454545454545453\n",
            "[start] [UNK] por ahí adentro [end] [start] solo espera ahí. [end]\n",
            "Score:0.4\n",
            "[start] quieres tocar la [UNK] y [UNK] [end] [start] ¿quieres jugar al escondite? [end]\n",
            "Score:0.36363636363636365\n",
            "[start] no podía creer que me las orejas [end] [start] no podía creer lo que oía. [end]\n",
            "Score:0.3695652173913043\n",
            "[start] el propósito [UNK] por todo su cuerpo [end] [start] el veneno se extendió por todo su cuerpo. [end]\n",
            "Score:0.3333333333333333\n",
            "[start] deberíamos seguir hacia silencio [end] [start] deberíamos hacer silencio. [end]\n",
            "Score:0.391304347826087\n",
            "[start] Él se puso cerca [end] [start] él se volteó. [end]\n",
            "Score:0.4\n",
            "[start] me llueve hacia las seis en punto [end] [start] me quedaré allí hasta las seis. [end]\n",
            "Score:0.31914893617021284\n",
            "[start] la mitad del [UNK] mientras [UNK] había [UNK] [end] [start] la mitad de las bananas en la cesta estaban podridas. [end]\n",
            "Score:0.23728813559322035\n",
            "[start] tom no pudo entender lo que realmente quería mary [end] [start] tom no pudo comprender que quería realmente mary. [end]\n",
            "Score:0.2857142857142857\n",
            "[start] ella cerró sus manos hacia adelante [end] [start] ella cerró lentamente su diario. [end]\n",
            "Score:0.3469387755102041\n",
            "\n",
            "BLEU score : 7.41/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20 new sentences accuracy"
      ],
      "metadata": {
        "id": "RCfEIAiVmMgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manualTest = [\n",
        "   (\"I love learning new languages.\", \"Me encanta aprender nuevos idiomas.\"),\n",
        "    (\"This is a beautiful day.\", \"Hoy es un día hermoso.\"),\n",
        "    (\"She sings very well.\", \"Ella canta muy bien.\"),\n",
        "    (\"They are going to the park.\", \"Van al parque.\"),\n",
        "    (\"The book is on the table.\", \"El libro está en la mesa.\"),\n",
        "    (\"We need to finish this project.\", \"Necesitamos terminar este proyecto.\"),\n",
        "    (\"He is a good friend.\", \"Él es un buen amigo.\"),\n",
        "    (\"I want to travel around the world.\", \"Quiero viajar por todo el mundo.\"),\n",
        "    (\"The cat is sleeping on the sofa.\", \"El gato está durmiendo en el sofá.\"),\n",
        "    (\"She cooks delicious food.\", \"Ella cocina comida deliciosa.\"),\n",
        "    (\"We have a lot of work to do.\", \"Tenemos mucho trabajo que hacer.\"),\n",
        "    (\"He speaks Spanish fluently.\", \"Él habla español con fluidez.\"),\n",
        "    (\"The concert starts at 7 PM.\", \"El concierto comienza a las 7 PM.\"),\n",
        "    (\"I need to buy some groceries.\", \"Necesito comprar algunas provisiones.\"),\n",
        "    (\"She is studying for her exam.\", \"Ella está estudiando para su examen.\"),\n",
        "    (\"They are watching a movie.\", \"Están viendo una película.\"),\n",
        "    (\"He plays the guitar very well.\", \"Él toca la guitarra muy bien.\"),\n",
        "    (\"We are going to the beach tomorrow.\", \"Vamos a la playa mañana.\"),\n",
        "    (\"The museum is closed on Mondays.\", \"El museo está cerrado los lunes.\"),\n",
        "    (\"I am going to visit my family.\", \"Voy a visitar a mi familia.\")\n",
        "]\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "testENTexts = [pair[0] for pair in manualTest]\n",
        "testSPTexts = [pair[1] for pair in manualTest]\n",
        "score = 0\n",
        "bleu  = 0\n",
        "for i in range(20):\n",
        "    candidate = decode_sequence(testENTexts[i])\n",
        "    reference = testSPTexts[i].lower()\n",
        "    print(candidate,reference)\n",
        "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu+=score\n",
        "    print(f\"Score:{score}\")\n",
        "print(f\"\\nBLEU score : {round(bleu,2)}/20\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFKUwo7bmSDG",
        "outputId": "f9f7bf4d-3757-4dd2-c6f2-616f61fe7be6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] me encanta aprender nuevo lengua [end] me encanta aprender nuevos idiomas.\n",
            "Score:0.3043478260869566\n",
            "[start] es un día hermosa [end] hoy es un día hermoso.\n",
            "Score:0.3870967741935484\n",
            "[start] ella canta muy bien [end] ella canta muy bien.\n",
            "Score:0.36363636363636365\n",
            "[start] ellos van al parque [end] van al parque.\n",
            "Score:0.30303030303030304\n",
            "[start] el libro está sobre la mesa [end] el libro está en la mesa.\n",
            "Score:0.3170731707317073\n",
            "[start] necesitamos que [UNK] este proyecto [end] necesitamos terminar este proyecto.\n",
            "Score:0.2653061224489796\n",
            "[start] Él es un buen amigo [end] él es un buen amigo.\n",
            "Score:0.36363636363636365\n",
            "[start] quiero andar por toda la hora [end] quiero viajar por todo el mundo.\n",
            "Score:0.3023255813953489\n",
            "[start] el gato está durmiendo en el sofá [end] el gato está durmiendo en el sofá.\n",
            "Score:0.34042553191489366\n",
            "[start] ella cocina comida de arroz [end] ella cocina comida deliciosa.\n",
            "Score:0.2682926829268293\n",
            "[start] tenemos un montón de trabajo por hacer [end] tenemos mucho trabajo que hacer.\n",
            "Score:0.2692307692307693\n",
            "[start] Él habla español con [UNK] [end] él habla español con fluidez.\n",
            "Score:0.325\n",
            "[start] el concierto empieza a las matemáticas de noche [end] el concierto comienza a las 7 pm.\n",
            "Score:0.2295081967213115\n",
            "[start] necesito comprar algunos libros [end] necesito comprar algunas provisiones.\n",
            "Score:0.3333333333333333\n",
            "[start] ella estudiando para que [UNK] en el examen [end] ella está estudiando para su examen.\n",
            "Score:0.2631578947368421\n",
            "[start] están viendo una película [end] están viendo una película.\n",
            "Score:0.41025641025641024\n",
            "[start] Él toca bien la guitarra muy bien [end] él toca la guitarra muy bien.\n",
            "Score:0.31914893617021284\n",
            "[start] vamos a ir a la playa mañana [end] vamos a la playa mañana.\n",
            "Score:0.2619047619047619\n",
            "[start] el museo está los lunes [end] el museo está cerrado los lunes.\n",
            "Score:0.35135135135135137\n",
            "[start] voy a visitar a visitar a mi familia [end] voy a visitar a mi familia.\n",
            "Score:0.24\n",
            "\n",
            "BLEU score : 6.22/20\n"
          ]
        }
      ]
    }
  ]
}